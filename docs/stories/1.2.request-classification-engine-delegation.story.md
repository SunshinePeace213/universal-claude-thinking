# Story 1.2: Request Classification Engine with Delegation Integration

## Status
Draft

## Story
**As a** user submitting requests to the system,
**I want** automatic detection of task complexity and type with intelligent routing,
**so that** the system can delegate my request to appropriate specialists through the 3-stage system.

## Acceptance Criteria
1. Classification engine detects A/B/C/D/E task types with >95% accuracy
2. Classification results feed into 3-stage delegation engine (keyword ‚Üí semantic ‚Üí PE fallback)
3. Type A (Simple/Direct) routes through fast keyword matching (<10ms)
4. Type B (Complex/Multi-step) uses semantic delegation with agent embeddings
5. Type C (Research) triggers R1 agent with confidence score >0.9
6. Type D (Web/Testing) routes to T1 with tool capability mapping
7. Type E (Debugging) activates A1 with reasoning chain requirements
8. Ambiguous classifications (confidence <0.7) route to PE for enhancement
9. Classification and delegation metrics are logged for optimization

## Prerequisites & Environment Setup

**Model Requirements:**
- Qwen3-Embedding-8B model (pre-installed by infrastructure team)
- Model path will be provided in environment configuration

**Package Installation (using uv):**
```bash
# Install required dependencies
uv pip install sentence-transformers chromadb aiohttp asyncio uvloop

# Verify installation
uv pip list | grep -E "sentence-transformers|chromadb|aiohttp"
```

**Configuration:**
- Embedding model path: Check `config/models.yaml` for pre-configured path
- Vector DB settings: Configure in `config/delegation.yaml`

**Note:** The Qwen3-Embedding-8B model is pre-provisioned. Do not attempt to download it manually.

## Tasks / Subtasks
- [ ] Task 1: Implement Request Classification Engine (AC: 1, 7, 8)
  - [ ] Create RequestClassifier class in `/src/core/atomic/classifier.py` [Source: architecture/7-layer-context-engineering-architecture.md#Layer1]
  - [ ] Implement A/B/C/D/E task type detection algorithms
  - [ ] Add confidence scoring mechanism with thresholds
  - [ ] Create classification metrics logging
  - [ ] Write unit tests for classification accuracy (target >95%)
  
- [ ] Task 2: Implement 3-Stage Hybrid Delegation Engine (AC: 2, 3, 4, 8)
  - [ ] Create HybridDelegationEngine class in `/src/delegation/engine.py` [Source: architecture/unified-project-structure.md#delegation]
  - [ ] Implement Stage 1: Fast keyword matching (<10ms) in `/src/delegation/keyword_matcher.py`
  - [ ] Implement Stage 2: Semantic matching with Qwen3-Embedding-8B in `/src/delegation/semantic_matcher.py`
  - [ ] Implement Stage 3: PE fallback mechanism in `/src/delegation/pe_fallback.py`
  - [ ] Create confidence scoring system in `/src/delegation/confidence_scorer.py`
  
- [ ] Task 3: Create Agent Capability Mapping (AC: 5, 6, 7)
  - [ ] Define agent capability matrix with primary triggers
  - [ ] Implement routing logic for Type C ‚Üí R1 (Research)
  - [ ] Implement routing logic for Type D ‚Üí T1 (Web/Testing)
  - [ ] Implement routing logic for Type E ‚Üí A1 (Debugging)
  - [ ] Create tool capability mapping for T1 agent
  
- [ ] Task 4: Integrate with Existing Atomic Foundation (AC: 1, 2)
  - [ ] Connect RequestClassifier with AtomicFoundation from Story 1.1
  - [ ] Feed AtomicAnalysis results into classification engine
  - [ ] Ensure quality scores influence classification confidence
  - [ ] Update atomic_validator.py hook to include classification
  
- [ ] Task 5: Implement Database Schema for Metrics (AC: 9)
  - [ ] Create delegation_metrics table schema [Source: architecture/data-architecture-storage-systems.md#DatabaseSchemaDesign]
  - [ ] Add classification_history table for tracking
  - [ ] Implement async logging for classification results
  - [ ] Create indexes for performance optimization
  
- [ ] Task 6: Create Performance Monitoring (AC: 3, 9)
  - [ ] Implement latency tracking for each delegation stage
  - [ ] Create metrics collection for classification accuracy
  - [ ] Add dashboard queries for delegation performance
  - [ ] Set up alerts for performance degradation
  
- [ ] Task 7: Write Comprehensive Tests (AC: all)
  - [ ] Create unit tests in `/tests/unit/test_classification.py` [Source: architecture/unified-project-structure.md#TestSuite]
  - [ ] Create unit tests in `/tests/unit/test_delegation.py`
  - [ ] Create integration tests in `/tests/integration/test_classification_delegation_flow.py`
  - [ ] Test keyword matching speed (<10ms requirement)
  - [ ] Test classification accuracy (>95% requirement)
  - [ ] Test confidence thresholds and fallback behavior

## Dev Notes

### Project Context
This story implements request classification and delegation integration, building on the Atomic Foundation from Story 1.1. It introduces the 3-stage hybrid delegation system that intelligently routes user requests to appropriate specialist sub-agents.

### Previous Story Insights
From Story 1.1 implementation:
- AtomicFoundation class successfully analyzes prompt structure with <500ms performance
- Quality scoring (1-10 scale) provides reliable prompt assessment
- Chain of Verification (CoVe) enhancement reduces hallucinations by 30-50%
- Claude Code hooks (atomic_validator.py, prompt_enhancer.py) are registered and functional
- Qwen3-Embedding-8B model is configured for embeddings (1536-dim vectors)

### Data Models
**Request Classification Types** [Source: architecture/agent-automatic-delegation-tool-seo-optimization.md#hybrid-delegation-framework]
```python
# Task Type Enumeration
class TaskType(Enum):
    TYPE_A = "simple_direct"      # Quick facts, simple fixes
    TYPE_B = "complex_multi_step" # Feature dev, architecture
    TYPE_C = "research_required"  # Current info, docs lookup
    TYPE_D = "web_testing"        # UI testing, browser tasks
    TYPE_E = "debugging_error"    # Bug fixes, troubleshooting

# Classification Result Model
class ClassificationResult:
    task_type: TaskType
    confidence: float  # 0.0-1.0
    reasoning: str
    suggested_agent: str
    delegation_method: str  # "keyword", "semantic", or "fallback"
```

**Database Schema** [Source: architecture/data-architecture-storage-systems.md#DatabaseSchemaDesign]
```sql
-- Delegation metrics tracking
CREATE TABLE delegation_metrics (
    id UUID PRIMARY KEY,
    request_id UUID NOT NULL,
    classification_type VARCHAR(20),
    confidence_score DECIMAL(3,2),
    delegation_method VARCHAR(20),
    selected_agent VARCHAR(50),
    stage1_latency_ms INTEGER,
    stage2_latency_ms INTEGER,
    stage3_latency_ms INTEGER,
    total_latency_ms INTEGER,
    success BOOLEAN,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_classification (classification_type),
    INDEX idx_agent (selected_agent)
);

-- Classification history for accuracy tracking
CREATE TABLE classification_history (
    id UUID PRIMARY KEY,
    prompt_hash TEXT,
    predicted_type VARCHAR(20),
    actual_type VARCHAR(20),
    confidence DECIMAL(3,2),
    correct BOOLEAN,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_accuracy (correct, created_at)
);
```

### API Specifications
**HybridDelegationEngine Core Methods** [Source: architecture/agent-automatic-delegation-tool-seo-optimization.md#three-stage-delegation-system]
```python
async def delegate(self, user_input: str) -> DelegationResult:
    """Main delegation method using 3-stage system."""
    
async def _keyword_match(self, user_input: str) -> Optional[str]:
    """Stage 1: Fast keyword matching (<10ms)."""
    
async def _semantic_match(self, user_input: str) -> DelegationResult:
    """Stage 2: Semantic similarity using embeddings."""
    
async def _pe_enhancement_fallback(self, user_input: str) -> DelegationResult:
    """Stage 3: Route to PE for clarification."""
```

### Component Specifications
**Delegation System Components** [Source: architecture/agent-automatic-delegation-tool-seo-optimization.md#hybrid-delegation-framework]
- `RequestClassifier` - Detects A/B/C/D/E task types
- `HybridDelegationEngine` - 3-stage delegation orchestrator
- `KeywordMatcher` - Fast pattern matching (Stage 1)
- `SemanticMatcher` - Embedding-based matching (Stage 2)
- `PEFallback` - Enhancement routing (Stage 3)
- `ConfidenceScorer` - Delegation confidence calculation

**Agent Capability Matrix** [Source: architecture/agent-automatic-delegation-tool-seo-optimization.md#agent-capability-mapping]
```yaml
PE (üîß): ["prompt enhancement", "input validation", "quality assessment"]
R1 (üîç): ["web search", "data gathering", "source verification"]
A1 (üß†): ["logical analysis", "problem solving", "reasoning chains"]
E1 (üìä): ["quality assessment", "validation", "error detection"]
T1 (üõ†Ô∏è): ["tool execution", "automation", "system interaction"]
W1 (üñãÔ∏è): ["content creation", "writing", "documentation"]
I1 (üó£Ô∏è): ["user interaction", "clarification", "communication"]
```

### File Locations
- Core classification: `/src/core/atomic/classifier.py` [Source: architecture/unified-project-structure.md#CoreDirectories]
- Delegation engine: `/src/delegation/` directory [Source: architecture/unified-project-structure.md#delegation]
- Tests: `/tests/unit/test_classification.py`, `/tests/unit/test_delegation.py` [Source: architecture/unified-project-structure.md#TestSuite]
- Integration tests: `/tests/integration/test_classification_delegation_flow.py`

### Testing Requirements
**Performance Constraints** [Story AC #3, #9]
- Keyword matching: <10ms latency requirement
- Semantic matching: 50-100ms acceptable range
- Classification accuracy: >95% target
- Confidence threshold: 0.7 for direct routing, <0.7 triggers PE fallback

**Coding Standards** [Source: architecture/coding-standards.md#CriticalCognitiveArchitectureRules]
- All cognitive functions must be pure functions with no side effects
- Type annotations required for all functions including returns
- Async-first design for I/O operations with timeout handling
- Test naming: `test_` prefix with descriptive names

### Technical Constraints
- Classification confidence thresholds: [Source: architecture/agent-automatic-delegation-tool-seo-optimization.md#three-stage-delegation-system]
  - Keyword matching: 0.9 confidence
  - Semantic matching: 0.7 minimum
  - PE fallback: Always 1.0 confidence
- Performance targets:
  - Stage 1 (Keyword): <10ms
  - Stage 2 (Semantic): 50-100ms
  - Stage 3 (PE): 100-200ms
- Embedding model: Qwen3-Embedding-8B (1536 dimensions)
- Vector similarity: Cosine similarity with 0.85 threshold

### Reference: Request Classification from CLAUDE-v3.md

**Classification Types (Priority Order: E‚ÜíD‚ÜíC‚ÜíB‚ÜíA):**
- **Type E** - Debugging/Error Resolution (reactive problem-solving)
- **Type D** - Web/Testing (specialized testing workflows)
- **Type C** - Research Required (information gathering)
- **Type B** - Complex/Multi-step (proactive development)
- **Type A** - Simple/Direct (basic information)

**Note:** This classification system ensures proper routing priority and delegation logic.

### Confidence Threshold Matrix

| Task Type | Min Confidence | Delegation Method | Fallback |
|-----------|---------------|-------------------|----------|
| TYPE_A | 0.95 | Keyword Match | Semantic |
| TYPE_B | 0.85 | Keyword/Semantic | PE Enhancement |
| TYPE_C | 0.90 | Semantic Match | PE Enhancement |
| TYPE_D | 0.85 | Semantic Match | PE Enhancement |
| TYPE_E | 0.80 | Keyword/Semantic | PE Enhancement |

**Note:** Confidence below threshold triggers next delegation stage or PE fallback.

### Example Request Classification Header

```
üìä REQUEST CLASSIFICATION ENGINE v1.2
=====================================
üéØ Classification: TYPE_C (Research Required)
üî¢ Confidence: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.92/1.0 [High]
   Threshold: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 0.70 (Met ‚úÖ)

üö¶ Routing Decision:
   ‚îî‚îÄ Keyword Match: ‚ùå No match found
   ‚îî‚îÄ Semantic Match: ‚úÖ Agent match found (0.92)
   ‚îî‚îÄ PE Fallback: ‚è∏Ô∏è Not triggered

ü§ñ Delegation Target: Agent/Sub-agent (Details in delegation result)
üìù Routing Method: Semantic Matching (Stage 2)

‚ö° Performance: 47ms total [‚úÖ Within 100ms target]
   ‚îú‚îÄ Classification: 12ms
   ‚îú‚îÄ Delegation: 35ms
   
üéñÔ∏è Status: ‚úÖ Complete
=====================================
```

## Testing

### Testing Standards
- Test file locations: [Source: architecture/unified-project-structure.md#TestSuite]
  - Unit tests: `/tests/unit/test_classification.py`, `/tests/unit/test_delegation.py`
  - Integration tests: `/tests/integration/test_classification_delegation_flow.py`
- Testing framework: pytest (Python project standard)
- Test naming convention: `test_` prefix with descriptive names
- Performance tests must verify latency constraints
- Classification accuracy tests must verify >95% accuracy target
- Mock external dependencies (embeddings) for unit tests
- Integration tests should test full classification ‚Üí delegation flow

## Further Reading (Optional)

The following external resources provide deeper understanding of concepts used in this story. These are optional references for developers wanting to learn more about the underlying techniques:

### Core Concepts
- **Hybrid Delegation Systems**: [Three-Stage Delegation Pattern](https://patterns.dev/posts/delegation-pattern) - Understanding multi-stage routing architectures
- **Text Classification**: [Building Production Classification Systems](https://huggingface.co/docs/transformers/tasks/sequence_classification) - Modern approaches to intent categorization
- **Semantic Similarity**: [Understanding Embedding-based Search](https://www.pinecone.io/learn/vector-similarity/) - Vector similarity for semantic matching

### Technologies & Tools
- **Qwen3-Embedding-8B**: [Model Card and Usage](https://huggingface.co/Qwen/Qwen3-Embedding-8B) - Official documentation for the embedding model, use Context7
- **mlx-community/Qwen3-Embedding-8B-4bit-DWQ**: [Model Card and Usage](https://huggingface.co/mlx-community/Qwen3-Embedding-8B-4bit-DWQ) - Official documentation for the embedding model by MLX, use Context7
- **ChromaDB Performance**: [Optimization Guide](https://docs.trychroma.com/guides/performance) - Achieving <100ms query performance, use Context7
- **Async Python**: [Real Python Async Guide](https://realpython.com/async-io-python/) - Best practices for concurrent operations, use Context7

### Implementation References
- **Confidence Calibration**: [Calibrating Neural Networks](https://arxiv.org/abs/1706.04599) - Ensuring reliable confidence scores
- **Multi-Stage Routing**: [Cascade Classification Systems](https://dl.acm.org/doi/10.1145/3442381.3449862) - Optimizing multi-stage decision systems
- **Few-Shot Text Classification**: [Context Engineering Foundations](https://github.com/davidkimai/Context-Engineering/blob/main/00_foundations/02_molecules_context.md) - Efficient classification with minimal examples

**Note:** All implementation details are in the story above. These resources provide background understanding only.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results